[{"title":"PyCharm连接教程","date":"2020-04-08T09:45:55.000Z","url":"/2020/04/08/PyCharm%E8%BF%9E%E6%8E%A5%E6%95%99%E7%A8%8B/","content":"Pycharm连接服务器社区版的pycharm不支持远程连接服务器的功能，需要使用专业版。 官网学生认证免费获得专业版 首先配置 Pycharm 服务器的代码同步，打开最上方工具栏处的 Tools -&gt; Deployment -&gt;Configuration，在弹出的窗口处点击左边的 + 添加一个部署配置，输入配置名 Name，Type 选择SFTP，然后确认。 配置远程服务器的 IP，端口，用户名和密码，Root Path是项目文件在远程服务器中的根目录 以下面的一条ssh命令为例： ssh mist@47.103.52.232 -p 48744 用户名为mist，ip(host)为47.103.52.232，端口为48744，密码是创建服务器时填写的。 Root path则需设置成/home/mist/下您自己项目的文件夹。 点击 Mappings，将 Local Path 设置为 Windows 下的工程目录，例如D:\\Projects\\ML-Project，自己视情况设定。将 Deployment path on server设置为远程服务器中的项目目录 点击 Excluded Paths 可以设置一些不想同步的目录，例如软件的配置文件目录等。 另外打开 Tools -&gt; Deployment -&gt; Options，将 Create Empty directories打上勾，要是指定的文件夹不存在，会自动创建。"},{"title":"VSCode连接教程","date":"2020-04-08T09:45:55.000Z","url":"/2020/04/08/VSCode%E8%BF%9E%E6%8E%A5%E6%95%99%E7%A8%8B/","content":"VSCode连接服务器教程官方教程 首先，在vscode商店内安装Remote-SSH与Remote-SSH:EDITING 插件 切换至ssh标签页，点击加号 随后，在弹出的框中完整复制服务器管理页面的ssh命令 选择ssh配置文件，一般回车选择默认即可 随后，输入创建服务器时您输入的密码 稍后就可以直接开始使用。 点击右侧的+窗口按钮，即可在新窗口中连接至服务器。 高级设置：公钥连接以下的内容为启用公钥连接，这样连接时就不需要输入密码。 打开命令行（Win下以管理员权限打开cmd），输入ssh-keygen -t rsa 然后会在.ssh文件下见到生成的公钥密钥 将id_rsa.pub文件上传至服务器 进入SSH配置目录， cd ~/.ssh ls 查看一下是否有一个名为authorized_keys的文件，如果没有就创建一个，然后把刚上传的id_rsa.pub中的内容附到authorized_keys文件中， touch authorized_keys cat ~/id_rsa.pub &gt;&gt; authorized_keys 随后，更改权限，确保公钥生效 chmod -R 600 authorized_keys 最后点击远程连接界面中的齿轮，打开配置文件（与上文提及的相同） 在config最后加入PasswordAuthentication no并保存，即可登录。 Host名可随意更改，避免冲突。"},{"title":"AI主播，在线解说：腾讯AI lab提出多模态语音合成模型DurIAN","date":"2020-02-10T11:45:55.000Z","url":"/2020/02/10/AI%E4%B8%BB%E6%92%AD%EF%BC%8C%E5%9C%A8%E7%BA%BF%E8%A7%A3%E8%AF%B4%EF%BC%9A%E8%85%BE%E8%AE%AFAI-lab%E6%8F%90%E5%87%BA%E5%A4%9A%E6%A8%A1%E6%80%81%E8%AF%AD%E9%9F%B3%E5%90%88%E6%88%90%E6%A8%A1%E5%9E%8BDurIAN/","content":"论文笔记：腾讯AI lab多模态语音合成模型DurIAN论文：DurIAN: Duration Informed Attention Network For Multimodal Synthesis，演示地址。 概述DurIAN是腾讯AI lab于19年9月发布的一篇论文，主体思想和FastSpeech类似，都是抛弃attention结构，使用一个单独的模型来预测alignment，从而来避免合成中出现的跳词重复等问题，不同在于FastSpeech直接抛弃了autoregressive的结构，而DurIAN相当于一个去除Attention的Tacotron2/Tacotron，因为没有autoregressive，FastSpeech有了百倍的加速。除此以外，FastSpeech使用一个train好的Transformer-TTS model作为alignment信息的来源，而DurIAN使用Forced-alignment工具作为alignment信息的来源。DurIAN还利用语音+人脸特征点的数据集，让DurIAN在合成语音的同时预测对应的人脸特征点信息，从而实现AI虚拟人说话的效果；由于拥有不同情感标签的数据集，DurIAN进行了监督的情感学习，可以细粒度调整合成语音的情感，总而言之，演示的效果十分惊人。（paper中还提出了一种Multi-Band WaveRNN的结构，笔者对vocoder技术不是很熟悉，在这里就不做阐述了） 模型架构模型整体结构如下图所示，去除Attention的方法大家可以参考FastSpeech阅读笔记和Tacotron的paper，本文主要说说paper中对中文的特殊处理办法和Style Code的部分。 处理中文字符为了可以让模型学到更多地韵律、停顿信息，DurIAN在text部分加入了特殊字符（之前看过别人做中文TTS，如果不手工在句子里面加间隔，合成出来的断句可能完全是不对的），一共加了四种不同的特殊字符，其中#S表示音位的间断，#1表示prosodic words的间断，#2表示prosodic phrase的间断，#3表示intonational phrase的间断，如下图。在encoder部分这些间断作为一个单独的vector，最后encoder output要去除表示这些间断的vector，因为Forced-alignment不会给这些vector做alignment，而在CBHG结构中已经考虑过上下文，这些间断的信息已经被包含在了其他vector中了，所以这里直接丢弃。 Style Codepaper中花了很大篇幅阐明了像VAE和GST这些unsupervised model中，对style信息进行提取所存在的问题，最大的问题就是提取出来的style不具有实际意义，很难被实际应用，所以paper中使用了supervised的方法，使用了包含多种情感标签的数据进行训练。在training的时候，每个情感标签进行one-hot编码乘以embedding，得到对应情感的embedding vector，在乘以一个learnable的Control Scale获得Style Code，Style Code在模型的两个部分进行concatenate，大家可以参考第一幅图。在inference阶段，可以调节one-hot编码，比如原本one-hot中表示开心是[1, 0, 0, 0]，而inference想要合成既有开心又有兴奋的语音，就可以改为[0.5, 0, 0.5, 0]，用这种方法，DurIAN可以对合成语音进行细粒度的情感控制。 总结这篇paper让我意识到“原来语音合成还可以这么玩”，合成的效果的确很惊艳，特别是配合AI虚拟人的时候。一句题外话，FastSpeech和DurIAN都借助了一个“teacher model”，可不可以做到直接训练呢？teacher model提供的alignment会对student model的performance有什么影响吗？"}]